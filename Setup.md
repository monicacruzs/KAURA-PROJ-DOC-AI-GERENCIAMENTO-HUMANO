# üõ†Ô∏è SETUP.md: Provisionamento e Desprovisionamento Azure CLI (KAURA)

Este documento detalha o passo a passo t√©cnico para provisionar e desprovisionar os recursos do Azure necess√°rios para o projeto **Assistente de Gerenciamento de Documentos Centrado no Ser Humano (Doc Intelligence no Azure)**, usando a linha de comando (Azure CLI).

**‚ö†Ô∏è ESTRAT√âGIA DE CUSTO (FINOPS):**
O projeto segue uma rigorosa pol√≠tica de **Custo Zero**. O provisionamento √© dividido em duas fases:
1.  **Fase 1 (Custo Zero - PaaS):** Cria√ß√£o do Grupo de Recursos e do Servi√ßo de IA (Document Intelligence) em Tier Gratuito (F0).
2.  **Fase 2 (Custo por Hora - IaaS):** Cria√ß√£o da M√°quina Virtual (VM) para execu√ß√£o do script, com a meta de execu√ß√£o e **desprovisionamento total em no m√°ximo 2 horas**.

---
## 1. FASE 1: PROVISIONAMENTO (CUSTO ZERO - PAAS)

Estes comandos criam o "container" do projeto e os servi√ßos de PaaS de custo zero. Eles devem ser executados no **Azure Cloud Shell** ou em um ambiente com o Azure CLI instalado.


### 1.1. Configura√ß√£o e Cria√ß√£o do Servi√ßo de IA

Copie e cole este bloco no seu terminal:

```bash
# -- 1. VARI√ÅVEIS --
# Defina o Resource Group (Container) e a Localiza√ß√£o (brazilsouth)
RESOURCE_GROUP_NAME="RG-KAURA-DOC-AI"
LOCATION="brazilsouth" 
AI_SERVICE_NAME="kaura-doc-ai-service-kaura" # Lembre-se: Deve ser √∫nico globalmente.

# 2. CRIA O GRUPO DE RECURSOS
echo "Criando o Grupo de Recursos: $RESOURCE_GROUP_NAME..."
az group create \
    --name $RESOURCE_GROUP_NAME \
    --location $LOCATION

# 3. CRIA O SERVI√áO DE DOCUMENT INTELLIGENCE (SKU F0 √â GRATUITO)
echo "Criando o Servi√ßo de Document Intelligence (SKU F0 - GR√ÅTIS)..."
az cognitiveservices account create \
    --name $AI_SERVICE_NAME \
    --resource-group $RESOURCE_GROUP_NAME \
    --location $LOCATION \
    --kind "FormRecognizer" \
    --sku "F0" \
    --yes
```

---

### 1.2. Cria√ß√£o e Configura√ß√£o do Key Vault (Melhoria de Seguran√ßa)

Este passo cria o Key Vault para armazenar a chave secreta do DI, seguindo as melhores pr√°ticas.

**1. Registrar o Provedor de Recursos:**
```bash
az provider register --namespace 'Microsoft.KeyVault'
```
**2. Cria√ß√£o do Key Vault:**
Utilizamos o SKU standard e desabilitamos a autoriza√ß√£o RBAC inicial para usarmos as pol√≠ticas de acesso legadas, que s√£o mais simples de configurar neste contexto.

üîë KeyVaultName: kvkauradocaisecprod002 (Nome que voc√™ usou em todo o CI/CD)

```bash
az keyvault create \
  --name "kvkauradocaisecprod002" \
  --resource-group "RG-KAURA-DOC-AI" \
  --location "brazilsouth" \
  --sku "standard" \
  --enable-rbac-authorization false
  ```
**3. Obter e Armazenar a Chave do Document Intelligence:**

Este √© o passo mais importante: a chave do DI sai do Portal e entra no Key Vault.

- Passo Manual: Obtenha a Key 1 do seu recurso AI.

- Armazenar no KV: Substitua [CHAVE_DOCUMENT_INTELLIGENCE_AQUI] pelo valor copiado:

```bash
az keyvault secret set \
  --vault-name "kvkauradocaisecprod002" \
  --name "document-intelligence-key" \
  --value "[CHAVE_DOCUMENT_INTELLIGENCE_AQUI]"
  ```
**4. Definir a Pol√≠tica de Acesso para Teste Local (Opcional):**

Este passo define uma pol√≠tica de acesso que permite √† sua conta de usu√°rio (n√£o o Service Principal) ler o segredo do Key Vault, sendo √∫til para testes de desenvolvimento local.

```bash
# 1. Obt√©m seu Object ID
OBJECT_ID=$(az ad signed-in-user show --query id -o tsv)
 
# 2. Define permiss√£o 'get' e 'list' de segredos no Key Vault para sua conta
az keyvault set-policy \
  --name "kvkauradocaisecprod002" \
  --object-id "$OBJECT_ID" \
  --secret-permissions get list
 ```

---

### 1.3. Obter Credenciais Finais (Endpoint e IDs)
Este passo √© necess√°rio para obter o Endpoint (URL) (que n√£o vai para o KV) e para documentar o Tenant/Subscription IDs, que s√£o necess√°rios para o CI/CD (OIDC) e para a fase inicial de teste na VM.

‚ö†Ô∏è ALERTA DE SEGURAN√áA: O resultado destes comandos deve ser anotado em um local seguro e JAMAIS salvo publicamente neste reposit√≥rio.

```bash
# 1. Obter o PONTO DE EXTREMIDADE (ENDPOINT)
echo "Anote o Ponto de Extremidade (AZURE_ENDPOINT):"
az cognitiveservices account show \
    --name $AI_SERVICE_NAME \
    --resource-group $RESOURCE_GROUP_NAME \
    --query endpoint -o tsv

# 2. Obter a CHAVE DE ACESSO (KEY1)
echo "Anote a Chave de Acesso (AZURE_KEY):"
az cognitiveservices account keys list \
    --name $AI_SERVICE_NAME \
    --resource-group $RESOURCE_GROUP_NAME \
    --query key1 -o tsv
```
## FASE 2: PROVISIONAMENTO IaaS (VM) E EXECU√á√ÉO (2 HORAS)
‚ö†Ô∏è ALERTA DE CUSTO: O custo da M√°quina Virtual Standard_B2s inicia no momento em que o comando az vm create √© executado.

2.1. Cria√ß√£o da VM e Rede

```bash
# -- 1. VARI√ÅVEIS DA VM --
VM_NAME="KAURA-VM-PROC-01"
VM_USERNAME="kaurauser"  # Nome de usu√°rio para SSH
VM_PASSWORD="SuaSenhaForteAqui123!" # *** SUBSTITUA PELA SUA SENHA FORTE ***

# 2. CRIA A REDE VIRTUAL (VNet) e Sub-rede
echo "Criando Rede Virtual..."
az network vnet create \
    --resource-group $RESOURCE_GROUP_NAME \
    --name VNET-KAURA \
    --address-prefix 10.0.0.0/16 \
    --subnet-name Subnet-VM \
    --subnet-prefix 10.0.0.0/24

# 3. CRIA O GRUPO DE SEGURAN√áA DE REDE (NSG) e abre a porta 22 (SSH)
echo "Criando Grupo de Seguran√ßa de Rede (NSG)..."
az network nsg create \
    --resource-group $RESOURCE_GROUP_NAME \
    --name NSG-KAURA-SSH

# Cria a regra para permitir acesso SSH
az network nsg rule create \
    --resource-group $RESOURCE_GROUP_NAME \
    --nsg-name NSG-KAURA-SSH \
    --name Allow-SSH \
    --protocol tcp \
    --direction Inbound \
    --priority 100 \
    --source-address-prefix Internet \
    --source-port-range "*" \
    --destination-address-prefix "*" \
    --destination-port-range 22

# 4. CRIA A M√ÅQUINA VIRTUAL (VM) - IN√çCIO DA COBRAN√áA
echo "Criando a VM (Ubuntu Server B2s) - CUSTO INICIADO..."
az vm create \
    --resource-group $RESOURCE_GROUP_NAME \
    --name $VM_NAME \
    --image UbuntuLTS \
    --size Standard_B2s \
    --admin-username $VM_USERNAME \
    --admin-password $VM_PASSWORD \
    --vnet VNET-KAURA \
    --subnet Subnet-VM \
    --nsg NSG-KAURA-SSH \
    --public-ip-sku Standard \
    --location $LOCATION

# 5. EXIBE O ENDERE√áO IP P√öBLICO (Para conex√£o via SSH)
echo "VM Criada! Obtendo o IP P√∫blico (Para SSH):"
az vm show -d --resource-group $RESOURCE_GROUP_NAME --name $VM_NAME --query publicIps -o tsv
```

### 2.2. Configura√ß√£o de Seguran√ßa: Criando o `.gitignore`

**‚ö†Ô∏è CRUCIAL: SEGURAN√áA**

Para evitar que chaves secretas do Azure, Tokens ou vari√°veis de ambiente sejam acidentalmente enviados ao GitHub, voc√™ deve criar um arquivo de exclus√£o.

**A√ß√£o:** Crie o arquivo **`.gitignore`** na **raiz** do seu reposit√≥rio com o seguinte conte√∫do:

Conte√∫do do .gitignore:

```bash
# Configura√ß√µes Essenciais de Seguran√ßa e Ambiente

# Vari√°veis de Ambiente e Chaves (Crucial para Azure Secrets!)
.env
.local
*.json.local

# Ambientes Python e Cache
__pycache__/
*.pyc
venv/
.pytest_cache/

# Arquivos de Sistema Ocultos
.DS_Store

# Sa√≠das do Projeto (O GitHub Actions pode gerar)
output/
dados/

```

### 2.3. Cria√ß√£o dos Arquivos de C√≥digo e Automa√ß√£o (CI/CD)

**Contexto:** Estes arquivos devem ser criados na sua VM (acessada via SSH) para que voc√™ possa envi√°-los ao GitHub. Utilize o editor de texto `nano` para criar/editar os arquivos. Este script √© o cora√ß√£o do projeto. Ele se conecta ao Azure Document Intelligence, processa o documento de teste (`assets/lista-material-escolar.jpeg`) e imprime os resultados.

#### 2.3.1. Script Python (`src/analyze_doc_ai.py`)

Este script √© o cora√ß√£o do projeto. Ele se conecta ao Azure Document Intelligence, processa o documento de teste, e imprime os resultados.

**A√ß√£o:** 1. Crie a pasta `src/`: `mkdir src`
2. Crie e edite o arquivo Python: `nano src/analyze_doc_ai.py`
3. Cole o c√≥digo e salve (CTRL+X, S, ENTER).

> üí° **Nota:** O c√≥digo abaixo √© um exemplo de *estrutura* para documenta√ß√£o. O c√≥digo **final e funcional** que est√° no reposit√≥rio √© mais robusto: ele cont√©m a l√≥gica **unificada** para analisar tanto o `prebuilt-layout` quanto o `prebuilt-invoice` e salva dois arquivos diferentes (`.txt` e `.json`) como Artefatos.

```python
# C√ìDIGO DO ANALYZE_DOC_AI.PY

import os
import argparse
import json
from azure.core.credentials import AzureKeyCredential
from azure.ai.formrecognizer import DocumentAnalysisClient
from dotenv import load_dotenv

# Carrega as vari√°veis de ambiente do arquivo .env (√∫til para desenvolvimento local,
# mas no GitHub Actions, os secrets AZURE_* s√£o usados diretamente)
load_dotenv()

# --- Configura√ß√µes de Ambiente ---
endpoint = os.environ.get("AZURE_FORM_RECOGNIZER_ENDPOINT")
key = os.environ.get("AZURE_FORM_RECOGNIZER_KEY")
VM_IP = os.environ.get("VM_IP_PUBLICO", "SUA_VM_IP_AQUI")

# Define as configura√ß√µes de caminho e extra√ß√£o para cada modelo
# --- Mapeamento de Campos e Configura√ß√£o de Modelos ---
MODEL_CONFIG = {
    "prebuilt-layout": {
        "description": "Extra√ß√£o de Layout e Texto Puro.",
        "path": "dados/documento-teste.jpeg", 
        "extract_fields": False,
        "output_file": "dados_layout_extraidos.txt" 
    },
    "prebuilt-invoice": {
        "description": "Extra√ß√£o de Campos de Fatura.",
        "path": "dados/fatura-teste.pdf",
        "extract_fields": {
            "InvoiceId": "ID da Fatura",
            "CustomerName": "Nome do Cliente",
            "InvoiceTotal": "Total da Fatura"
        },
        "output_file": "dados_fatura_extraidos.json"
    }
}

def analyze_document(model_id, document_path):
    """
    Fun√ß√£o unificada para an√°lise de documentos com base no model_id.
    """
    if not endpoint or not key:
        print("ERRO: O ENDPOINT ou KEY n√£o foi encontrado nas vari√°veis de ambiente.")
        return

    config = MODEL_CONFIG.get(model_id)
    if not config:
        print(f"ERRO: Modelo '{model_id}' n√£o suportado ou n√£o configurado.")
        print(f"Modelos suportados: {list(MODEL_CONFIG.keys())}")
        return

    # 1. Autentica√ß√£o no Azure
    document_analysis_client = DocumentAnalysisClient(
        endpoint=endpoint, credential=AzureKeyCredential(key)
    )

    # Verifica a exist√™ncia do arquivo no workspace do GitHub Actions
    if not os.path.exists(document_path):
        print(f"ERRO: Arquivo de documento n√£o encontrado no caminho: {document_path}")
        print("\n*** A√á√ÉO NECESS√ÅRIA ***")
        print(f"Confirme se o arquivo de teste ({document_path}) foi comitado para a pasta 'dados/' do reposit√≥rio.")
        return

    print(f"Conectado ao Azure. Analisando documento: {document_path} usando modelo '{model_id}'...")

    try:
        # 2. Executa a an√°lise
        with open(document_path, "rb") as f:
            poller = document_analysis_client.begin_analyze_document(
                model_id, document=f.read()
            )
            result = poller.result()

        print(f"\n--- Resultado da An√°lise ({config['description']}) ---")
        
        # ------------------------------------------------------------------
        # 3. L√≥gica de Extra√ß√£o e Output (Modelos Estruturados: Projeto 2 - Faturas)
        # ------------------------------------------------------------------
        if config['extract_fields']:
            dados_extraidos = {}
            
            if result.documents:
                doc = result.documents[0]
                
                for campo_nome, campo_descricao in config['extract_fields'].items():
                    campo = doc.fields.get(campo_nome)
                    
                    valor = None
                    confianca = "N/A"
                    
                    if campo and campo.value is not None:
                        confianca = f"{campo.confidence:.2f}"
                        
                        # Corre√ß√£o para o erro 'value_currency' (Projeto 2)
                        if campo_nome == "InvoiceTotal" and hasattr(campo, 'value_currency') and campo.value_currency:
                            valor_currency = campo.value_currency
                            valor = f"{valor_currency.amount} {valor_currency.currency_code or valor_currency.currency_symbol}"
                        else:
                            valor = str(campo.value)
                        
                        dados_extraidos[campo_nome] = {
                            "Valor": valor,
                            "Confianca": float(confianca) 
                        }
                        
                        print(f"**{campo_descricao}** ({campo_nome}): {valor or 'N√£o Encontrado'} (Confian√ßa: {confianca})")

                # --- 4. Salvar em JSON (para Artefato do Projeto 2) ---
                if config['output_file']:
                    with open(config['output_file'], "w", encoding="utf-8") as f:
                        json.dump(dados_extraidos, f, indent=4, ensure_ascii=False)
                    print(f"\n‚úÖ Resultado da extra√ß√£o salvo para Artefato: {config['output_file']}")
            
            else:
                print(f"Nenhum documento do tipo '{model_id}' detectado no arquivo.")

        # ------------------------------------------------------------------
        # 3. L√≥gica de Extra√ß√£o e Output (Modelos de Layout: Projeto 1 - Layout/OCR)
        # ------------------------------------------------------------------
        else: # Entra aqui se config['extract_fields'] √© False
            output_text = ""
            # 1. Coleta e Imprime o Texto
            for page in result.pages:
                for line in page.lines:
                    output_text += line.content + "\n" # Acumula o texto
                    print(line.content) # Imprime no log
            
            # 2. Salvar em TXT (para Artefato do Projeto 1)
            if config['output_file']:
                with open(config['output_file'], "w", encoding="utf-8") as f:
                    # Salva a string completa no arquivo TXT
                    f.write(output_text) 
                print(f"\n‚úÖ Resultado do layout salvo para Artefato: {config['output_file']}")
        
        print("---------------------------------------")

    except Exception as e:
        print(f"\nERRO DURANTE A AN√ÅLISE DO DOCUMENTO: {e}")

if __name__ == "__main__":
    # --- Configura√ß√£o do Argument Parser ---
    parser = argparse.ArgumentParser(
        description="Script unificado para an√°lise de documentos usando o Azure Document Intelligence."
    )
    
    # Argumento obrigat√≥rio para especificar o modelo
    parser.add_argument(
        '--model-id',
        required=True,
        choices=list(MODEL_CONFIG.keys()),
        help=f"ID do modelo do Azure Document Intelligence a ser utilizado. Op√ß√µes: {list(MODEL_CONFIG.keys())}"
    )
    
    args = parser.parse_args()
    
    # Define o caminho do documento com base no modelo
    path = MODEL_CONFIG[args.model_id]["path"]
    
    # Inicia a an√°lise
    analyze_document(args.model_id, path)

# FIM DO C√ìDIGO PYTHON
```
#### 2.3.2. Arquivo de Workflow (.github/workflows/main.yml)

Este arquivo define o pipeline que o GitHub Actions executar√° a cada `git push`, executando os dois projetos de forma **independente** para isolamento de testes e gera√ß√£o de dois Artefatos.

**A√ß√£o:** 1. Crie a pasta do workflow: `mkdir -p .github/workflows` 
2. Crie e edite o arquivo YAML: `nano .github/workflows/main.yml` 
3. Cole o c√≥digo abaixo e salve (CTRL+X, S, ENTER).

```yaml
name: Document AI Execution - CI

on:
  push:
    branches:
      - main

jobs:
  # --- JOB 1: PROJETO 2 - AN√ÅLISE DE FATURAS (JSON) ---
  analyze-fatura:
    name: Projeto 2 - Analisar Faturas
    runs-on: ubuntu-latest
    env:
      AZURE_FORM_RECOGNIZER_ENDPOINT: ${{ secrets.AZURE_FORM_RECOGNIZER_ENDPOINT }}
      AZURE_FORM_RECOGNIZER_KEY: ${{ secrets.AZURE_FORM_RECOGNIZER_KEY }}
    steps:
      - name: 1. Checkout code
        uses: actions/checkout@v4
      - name: 2. Set up Python and install dependencies
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
      - name: 3. Run pip install -r requirements.txt
        run: pip install -r requirements.txt
          
      - name: 4. Execute Fatura Analysis Script
        run: python3 src/analyze_doc_ai.py --model-id prebuilt-invoice
    
      # Upload do Artefato JSON
      - name: 5. Upload Output Artifact (JSON) üì¶
        uses: actions/upload-artifact@v4
        with:
          name: kaura-proj2-fatura-output-${{ github.run_id }}
          path: dados_fatura_extraidos.json
          
  # --- JOB 2: PROJETO 1 - AN√ÅLISE DE LAYOUT (TXT) ---
  analyze-layout:
    name: Projeto 1 - Analisar Layout/OCR
    runs-on: ubuntu-latest
    env:
      AZURE_FORM_RECOGNIZER_ENDPOINT: ${{ secrets.AZURE_FORM_RECOGNIZER_ENDPOINT }}
      AZURE_FORM_RECOGNIZER_KEY: ${{ secrets.AZURE_FORM_RECOGNIZER_KEY }}
    steps:
      - name: 1. Checkout code
        uses: actions/checkout@v4
      - name: 2. Set up Python and install dependencies
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
      - name: 3. Run pip install -r requirements.txt
        run: pip install -r requirements.txt
          
      - name: 4. Execute Layout Analysis Script
        run: python3 src/analyze_doc_ai.py --model-id prebuilt-layout
        
      # Upload do Artefato TXT
      - name: 5. Upload Output Artifact (TXT) üì¶
        uses: actions/upload-artifact@v4
        with:
          name: kaura-proj1-layout-output-${{ github.run_id }}
          path: dados_layout_extraidos.txt
   
   ``` 

### 2.4. Envio do C√≥digo e In√≠cio do Pipeline (Git Push)

Agora que todos os arquivos necess√°rios (c√≥digo em `src/`, workflow em `.github/`, depend√™ncias em `requirements.txt`, e o arquivo de seguran√ßa `.gitignore`) est√£o prontos, o √∫ltimo passo √© envi√°-los ao GitHub para disparar o pipeline de CI/CD.

Para enviar seu c√≥digo da VM para o GitHub e disparar o pipeline de CI/CD, voc√™ precisa se autenticar e configurar quem est√° fazendo o commit.

**‚ö†Ô∏è Autentica√ß√£o (SENHA):** A senha usada no `git push` n√£o √© sua senha do GitHub, mas sim o seu **Personal Access Token (PAT)**.

> **üí° Como Obter/Gerar o PAT:** Se voc√™ n√£o souber como gerar ou configurar seu PAT, consulte a se√ß√£o **FASE 4: Troubleshooting**, item 2 (**"Erro de Permiss√£o do GitHub"**) para o passo a passo completo. Lembre-se de dar a permiss√£o (`scope`) de **`workflow`** ao seu token.

1.  **Configurar Identidade Global (Uma Vez por VM):**

    ```bash
    # Substitua pelas suas credenciais do GitHub
    git config --global user.email "SEU_EMAIL_GITHUB"
    git config --global user.name "SEU_USUARIO_GITHUB"
    ```

2.  **Enviar para o GitHub (Dispara o CI/CD!):**

    ```bash
    # -- 1. Adicionar todos os arquivos ao rastreamento do Git --
    git add .

    # -- 2. Criar o commit de finaliza√ß√£o do ambiente --
    git commit -m "feat: Adicionado pipeline CI/CD, script Python e documentacao final."

    # -- 3. Enviar. Use o PAT como a senha quando solicitado --
    git push origin main 
    ```
Em Resumo:

| A√ß√£o | Contexto de Execu√ß√£o | Documenta√ß√£o no SETUP.md |
| :--- | :--- | :--- |
| **Cria√ß√£o/Edi√ß√£o de Arquivos** | Dentro da VM (`nano`) | Instru√ß√£o `nano` adicionada na FASE 2.3. |
| **`git clone`** | Dentro da VM (via SSH) | Instru√ß√£o de `git clone` na FASE 2.1. |
| **`git push`** | Dentro da VM (via SSH) | Instru√ß√£o de `git push` na FASE 2.4. |
| **Reposit√≥rio** | Criado previamente no GitHub Web | Subentendido pelo `git clone` e `git push`. |

---
## 4. ‚öôÔ∏è Projeto KAURA-DOC-AI-CUSTOM (Modelo Customizado)

Este projeto foca no treinamento de um modelo customizado para extrair dados de documentos n√£o-padr√£o espec√≠ficos do neg√≥cio (ex: formul√°rio X, contrato Y).

### 4.1. üíæ Configura√ß√£o do Azure Storage para Treinamento

O modelo customizado requer dados de treinamento armazenados em um cont√™iner espec√≠fico no Azure Blob Storage.

| Recurso | Tipo | Descri√ß√£o |
| :--- | :--- | :--- |
| **Conta de Storage** | Blob Storage (v2) | A mesma conta de Storage utilizada para entrada/sa√≠da de documentos (FinOps: Custo Zero). |
| **Cont√™iner** | `kaura-training-data` | Cont√™iner dedicado para armazenar os documentos de treinamento rotulados. |
| **Conte√∫do** | `*.pdf`, `*.jpg`, `*.png` + arquivos `.json` de r√≥tulo | M√≠nimo de **5 documentos** rotulados por tipo de documento customizado. |

### 4.2. ü§ñ Processo de Treinamento no Document Intelligence Studio

O treinamento √© realizado manualmente no Azure AI Document Intelligence Studio.

1.  **Acessar o Studio:** Navegar para o [Azure AI Document Intelligence Studio](https://formrecognizer.appliedai.azure.com/studio).
2.  **Criar Projeto:**
    * Selecionar **Modelos customizados** > **Criar um projeto**.
    * Ligar o projeto ao Recurso do Document Intelligence e ao Cont√™iner (`kaura-training-data`).
3.  **Rotulagem:** Fazer ou revisar a rotulagem dos campos no conjunto de documentos.
4.  **Treinamento:** Clicar em **Treinar**.
    * **Definir `Model ID` (Crucial):** O ID deve seguir o padr√£o `kaura-custom-seunome-vN` (ex: `kaura-custom-contrato-v1`).
    * **Modo de Treinamento:** Usar **Template** (para <10 docs e consistentes) ou **Neural** (para >10 docs e variados).
    * **Sa√≠da:** O `Model ID` treinado deve ser registrado no **Key Vault** como segredo para uso pelo pipeline de CI/CD.

### 4.3. üîê Regras de Permiss√£o de Acesso (RBAC)

Para que o Recurso do Document Intelligence possa **ler** os documentos do Storage para o treinamento, √© necess√°ria uma atribui√ß√£o de fun√ß√£o (Role-Based Access Control - RBAC).

| Principal (Quem precisa da permiss√£o) | Escopo (Onde a permiss√£o se aplica) | Fun√ß√£o (Qual permiss√£o √© concedida) | Descri√ß√£o |
| :--- | :--- | :--- | :--- |
| **Identidade Gerenciada do Recurso Document Intelligence** | Conta de Azure Storage | **Blob Storage Data Reader** | Permite que o servi√ßo leia os blobs (documentos e r√≥tulos) necess√°rios para o treinamento. |
| **Usu√°rio/ML Engineer (Para rotulagem e treinamento manual)** | Conta de Azure Storage | **Storage Blob Data Contributor** | Permite o upload/download de documentos de treinamento e arquivos de r√≥tulo (`.json`). |

**Passos para Configurar:**
1.  V√° para a Conta de Storage.
2.  Acesse **Controle de Acesso (IAM)**.
3.  Clique em **Adicionar** > **Adicionar atribui√ß√£o de fun√ß√£o**.
4.  Selecione a fun√ß√£o **Blob Storage Data Reader**.
5.  Em **Membros**, selecione **Identidade gerenciada** e escolha o Recurso de Document Intelligence.

**Nota FinOps (Custo Zero):** A permiss√£o √© tempor√°ria para o treinamento, mas a Identidade Gerenciada √© a forma mais segura e recomendada de acesso.

üèóÔ∏è SETUP.md: Provisionamento de Infraestrutura (FinOps e MLOps)
Este guia detalha os passos para provisionar a infraestrutura e configurar as permiss√µes, garantindo o m√≠nimo custo (FinOps) e a rastreabilidade (MLOps).

üìù Pr√©-requisito
Assuma que o Resource Group (RG-KAURA-DOC-AI) e o recurso de Document Intelligence (kaura-doc-ai-service-05) j√° est√£o criados.

Passo 1: Ativa√ß√£o da Identidade Gerenciada (Passo OBRIGAT√ìRIO)
A identidade gerenciada deve ser ativada antes de configurar o RBAC.

Navegue para o seu recurso de Document Intelligence: kaura-doc-ai-service-05.

No menu lateral, clique em Identity.

Na aba System assigned, mude o status para On (Ligado) e clique em Save.

Passo 2: Cria√ß√£o da Conta de Storage e Cont√™iner (FinOps)
Devido ao erro persistente SubscriptionNotFound no Azure CLI, este passo ser√° executado via Portal Azure.

Crie a Conta de Storage kauradocaitrg002 no Portal Azure.

Project details: Use o Resource Group RG-KAURA-DOC-AI.

Instance details:

Storage account name: kauradocaitrg002.

Region: South America - Brazil South.

Redundancy (FinOps): Configure a Redundancy como Locally-redundant storage (LRS).

Ap√≥s a cria√ß√£o, navegue at√© a Conta de Storage kauradocaitrg002 e v√° para Containers.

Crie o cont√™iner de treinamento: kaura-training-data (N√≠vel de acesso Private).

Passo 3: Atribui√ß√£o de Permiss√µes RBAC (IAM)
Este passo concede permiss√£o de leitura ao Document Intelligence para acessar os documentos.

Navegue para a Conta de Storage kauradocaitrg002.

V√° para Access control (IAM) e clique em + Add role assignment.

Role (Fun√ß√£o): Selecione Storage Blob Data Reader.

Member (Membro):

Selecione Managed identity.

Pesquise e selecione o recurso kaura-doc-ai-service-05 (Document Intelligence).

Finalize a atribui√ß√£o em Review + assign.

üíæ Passo 4: Upload dos Dados de Treinamento
Com a infraestrutura pronta, carregue os 9 documentos PDF para o cont√™iner.

Navegue para a Conta de Storage kauradocaitrg002 > Containers.

Clique no cont√™iner kaura-training-data.

Utilize o bot√£o Upload para carregar os seus 9 documentos PDF (incluindo o documento de teste de robustez com a simula√ß√£o de mancha de caf√©).

A infraestrutura est√° completa.

## Ap√≥s o Treino e Teste no Document Intelligence Studio vamos integrar a chamada para uma API

A adapta√ß√£o √© m√≠nima. O principal a ser feito √©:

1. Adicionar o modelo kaura-custom-viagem-v4 ao dicion√°rio MODEL_CONFIG.
2. Incluir a l√≥gica de extra√ß√£o para os 6 campos dentro da fun√ß√£o analyze_document.
3. Configurar o GitHub Actions

---

## C√≥digo Python Adaptado (`analyze_doc_ai.py`)

Abaixo est√° o c√≥digo atualizado. Substitua todo o seu dicion√°rio `MODEL_CONFIG` e a l√≥gica de extra√ß√£o de campos dentro da fun√ß√£o `analyze_document`.

### 1. Modifica√ß√£o do `MODEL_CONFIG`
Adicione a configura√ß√£o do seu modelo personalizado `kaura-custom-viagem-v4` ao dicion√°rio `MODEL_CONFIG`, juntamente com os 6 campos que voc√™ treinou:

Python
```Python
# .... (Inicio do Codigo) ...
# Define as configura√ß√µes de caminho e extra√ß√£o para cada modelo... (resto do c√≥digo omitido)
MODEL_CONFIG = {
   
    "prebuilt-layout": {
        "description": "Extra√ß√£o de Layout e Texto Puro.",
        "path": "dados/documento-teste.jpeg", 
        "extract_fields": False,
        "output_file": "outputs/dados_layout_extraidos.txt" 
    },
    "prebuilt-invoice": {
        "description": "Extra√ß√£o de Campos de Fatura.",
        "path": "dados/fatura-teste.pdf",
        "extract_fields": {
            "InvoiceId": "ID da Fatura",
            "CustomerName": "Nome do Cliente",
            "InvoiceTotal": "Total da Fatura"
        },
        "output_file": "outputs/dados_fatura_extraidos.json"
    },
    "kaura-custom-viagem-v4": {
        "description": "Extra√ß√£o de Campos Customizados de Viagem (Neural v4).",
        "path": "dados/documento_viagem_teste.pdf", # Crie um novo PDF de teste nesta pasta
        "extract_fields": {
            "Nome_do_Colaborador": "Nome",
            "Centro_de_Custo": "Centro de Custo",
            "Data_de_Inicio_da_Viagem": "In√≠cio da Viagem",
            "Data_de_Fim_da_Viagem": "Fim da Viagem",
            "Valor_Total_Aprovado": "Valor Total",
            "Status_de_Aprovacao": "Status de Aprova√ß√£o"
        },
        "output_file": "outputs/dados_viagem_extraidos.json" 
    }
}
# ... (resto do c√≥digo) ...

```

**A√ß√£o:** Lembre-se de colocar um documento de teste de viagem (por exemplo, o `KAURA_03.pdf`) na sua pasta `dados/` e renome√°-lo para `documento_viagem_teste.pdf` ou ajustar o campo `path`no `MODEL_CONFIG`.

### 2. Modifica√ß√£o da L√≥gica de Extra√ß√£o
Substitua o trecho `... (L√≥gica de loop e extra√ß√£o dos campos Try ... Except) ...` dentro da fun√ß√£o `analyze_document` 

Python
```Python
 try:
    # 2. Executa a an√°lise (Abre o arquivo e inicia o poller)
        with open(document_path, "rb") as f:
            poller = document_analysis_client.begin_analyze_document(
                model_id, document=f.read()
            )
            result = poller.result() # Esta linha pode gerar exce√ß√µes de rede/API
            
        print(f"\n--- Resultado da An√°lise ({config['description']}) ---")

        # ------------------------------------------------------------------
        # 3. L√≥gica de Extra√ß√£o e Output (UNIFICADA)
        # ------------------------------------------------------------------

        # Se for um modelo que extrai campos estruturados (Viagem ou Fatura)
        if config['extract_fields']:
            dados_extraidos = {}
            
            if result.documents:
                doc = result.documents[0]
                
                # --- L√≥gica de loop e extra√ß√£o dos campos ---
                print("Extraindo os campos do documento...")
                
                # Itera sobre os campos definidos no MODEL_CONFIG
                for field_name, friendly_name in config['extract_fields'].items():
                    field = doc.fields.get(field_name)
                    
                    valor = None
                    confianca = 0.0
                    
                    if field:
                        valor = field.value
                        confianca = field.confidence if field.confidence is not None else 0.0
                        
                        # Trata objetos de valor (como datas, n√∫meros ou objetos complexos)
                        if hasattr(valor, 'isoformat'): # Trata datas/tempos
                            valor = valor.isoformat()
                        elif hasattr(valor, 'text') and valor.text is not None:
                             valor = valor.text
                        elif isinstance(valor, dict): # Trata casos em que o valor √© um dicion√°rio
                             # Geralmente usamos apenas o texto extra√≠do para simplificar
                            valor = field.value.text
                            
                        
                        # Adiciona ao dicion√°rio de sa√≠da (dentro do IF)
                        dados_extraidos[field_name] = {
                            "valor": valor,
                            "confianca": round(confianca, 2)
                        }
                        
                        # Imprime no console para debug
                        print(f"  {friendly_name}: {valor} (Confian√ßa: {round(confianca, 2)})")
                            
                    else: # Este else est√° AGORA na indenta√ß√£o correta, correspondendo ao 'if field:'
                        print(f"  {friendly_name}: (N√£o encontrado)")
                        dados_extraidos[field_name] = {"valor": None, "confianca": "0.00"} # Adiciona ao dicion√°rio mesmo que n√£o encontrado

                # --- 4. Salvar em JSON (para Artefato) ---
                if config['output_file']:
                    # Cria a pasta 'outputs/' se n√£o existir (garantindo que o salvamento funcione)
                    os.makedirs('outputs/', exist_ok=True)
                    with open(config['output_file'], "w", encoding="utf-8") as f:
                        json.dump(dados_extraidos, f, indent=4, ensure_ascii=False)
                    print(f"\n‚úÖ Resultado da extra√ß√£o salvo para Artefato: {config['output_file']}")
                    
            else:
                print(f"Nenhum documento do tipo '{model_id}' detectado no arquivo.")
                
        # Se for um modelo de Layout/OCR (TXT)
        else: # Este else est√° no mesmo n√≠vel do 'if config['extract_fields']:' inicial
            output_text = ""
            
            # --- L√≥gica de loop e extra√ß√£o de texto (Layout) ---
            if result.pages:
                for page in result.pages:
                    if page.lines:
                        # Extrai o texto de cada linha e adiciona quebra de linha
                        output_text += "\n".join([line.content for line in page.lines]) + "\n"
            
            print("Conte√∫do de texto extra√≠do com sucesso.")

            # --- 4. Salvar em TXT (para Artefato) ---
            if config['output_file']:
                os.makedirs('outputs/', exist_ok=True)
                with open(config['output_file'], "w", encoding="utf-8") as f:
                    f.write(output_text)  
                print(f"\n‚úÖ Resultado do layout salvo para Artefato: {config['output_file']}")
                
        print("---------------------------------------")
       
    except Exception as e:
        print(f"\nERRO DURANTE A AN√ÅLISE DO DOCUMENTO: {e}")

# ... (fim da fun√ß√£o analyze_document) ...

```

## 3. Como Configurar o GitHub Actions
Para que o GitHub Actions consiga rodar o script e acessar o Key Vault, voc√™ precisa garantir que a **Identidade do GitHub** (o Service Principal) tenha permiss√£o de **leitura (Get Secret)** no seu Key Vault.

O pr√≥ximo passo seria:

1. **Criar/Verificar o Service Principal:** Garanta que o seu Service Principal (o "usu√°rio" do Azure para o GitHub) est√° criado e sincronizado.
2. **Atribuir Permiss√£o no Key Vault:** Ir no seu Key Vault (`kvkauradocaisecprod002`) e adicionar a permiss√£o "Get" (Obter) de Secrets para o Service Principal.
3. **Configurar o Workflow:** Ajustar o arquivo `.yml `do GitHub Actions para incluir o passo que executa o python `analyze_doc_ai.py --model-id kaura-custom-viagem-v4`.

**üöÄ Fluxo de A√ß√£o: Do C√≥digo ao GitHub Actions**
Para que seu script `analyze_doc_ai.py` funcione no GitHub Actions, a principal barreira √© a autentica√ß√£o para acessar o Key Vault (`kvkauradocaisecprod002.vault.azure.net`).

O fluxo ideal √©:

1. Otimiza√ß√£o do Setup de Seguran√ßa (Melhor Pr√°tica): Configurar o GitHub para usar a Workload Identity Federation para se conectar ao Azure. Isso elimina a necessidade de armazenar um segredo de Longa Dura√ß√£o (Service Principal Key) no GitHub, usando identidades de curta dura√ß√£o baseadas em certificados OpenID Connect (OIDC).
2. Permiss√£o no Key Vault: Dar ao "usu√°rio" do GitHub (a Workload Identity) permiss√£o para ler (Get) o segredo `document-intelligence-key`.
3. Execu√ß√£o do Workflow: Adicionar o passo no arquivo `.yml` que executa o `python analyze_doc_ai.py --model-id kaura-custom-viagem-v4`.

### Configurar a Identidade OIDC
Para manter a seguran√ßa e a automa√ß√£o, vamos configurar a conex√£o segura via OIDC.

Voc√™ precisa de dois comandos principais no Azure CLI para criar a identidade que o GitHub vai usar. **Voc√™ consegue rodar estes comandos no seu terminal, autenticado via** `az login`?

**1. Atribui√ß√£o da Fun√ß√£o de Acesso**
Vamos dar permiss√£o de leitor e de acesso ao Key Vault para a sua identidade de pipeline (substitua a `{resource-group-name} `pelo nome do seu grupo de recursos, onde o Key Vault e o Document Intelligence est√£o):

Bash
```Bash
# Se o seu Service Principal j√° existe, voc√™ pode pular este comando
# Se n√£o, substitua {resource-group-name} pelo nome correto
az role assignment create --role "Reader" --assignee <CLIENT_ID_DO_SERVICE_PRINCIPAL> --scope /subscriptions/<SUBSCRIPTION_ID>/resourceGroups/{resource-group-name}

# D√™ permiss√£o de acesso ao Key Vault (GET Secrets) para o Service Principal
# Substitua o nome do Key Vault e o ID do Service Principal
az keyvault set-policy --name kvkauradocaisecprod002 --secret-permissions get --spn <CLIENT_ID_DO_SERVICE_PRINCIPAL>
```

## 3. FASE 3: DESPROVISIONAMENTO E ESTRAT√âGIA FINOPS (CUSTO ZERO ESTRUTURAL)

Esta √© a estrat√©gia de **FinOps** que garante o custo zero para o projeto. Ela elimina os √∫nicos recursos que geram cobran√ßa persistente (VM, Disco de S.O. e IP P√∫blico Standard), mantendo o servi√ßo de **Document Intelligence F0 (GR√ÅTIS)** para futuros testes.

* **Recursos Mantidos (Custo Zero):** Document Intelligence F0, VNet, NSG.
* **Recursos Exclu√≠dos (Custo Eliminado):** M√°quina Virtual (VM), Disco do S.O. e IP P√∫blico Standard.

### 3.1. A√ß√£o de Custo Zero (M√©todo Recomendado: Portal do Azure)

Devido √† inconsist√™ncia do Azure CLI em excluir recursos de armazenamento e rede, o m√©todo mais seguro para garantir o Custo Zero √© a exclus√£o manual via Portal:

1.  **Excluir a VM:** Navegue at√© **Virtual Machines** e delete `KAURA-VM-PROC-01`. (Se ela j√° estiver como `Stopped (deallocated)`, a cobran√ßa de computa√ß√£o j√° parou).
2.  **Excluir o IP P√∫blico:** Navegue at√© **Public IP addresses** e delete o recurso `KAURA-VM-PROC-01PublicIp`. (Este √© o √∫ltimo custo de rede).
3.  **Excluir o Disco:** Navegue at√© **Disks** e delete o disco de S.O. (`KAURA-VM-PROC-01_OsDisk_1_...`) se ele n√£o tiver sido exclu√≠do automaticamente. (Este √© o √∫ltimo custo de armazenamento).

> ‚ÑπÔ∏è **DICA:** Voc√™ pode verificar a lista final de recursos no Resource Group `RG-KAURA-DOC-AI` para garantir que apenas os itens de Custo Zero (Document Intelligence, VNet, NSG) permane√ßam.

> üí° **Nota Arquitetural (SSH e Custo Zero):** O SSH e o IP P√∫blico **foram eliminados** porque a execu√ß√£o do c√≥digo foi migrada para o **GitHub Actions (CI/CD)**. O CI/CD usa **runners ef√™meros** (m√°quinas virtuais tempor√°rias gerenciadas pelo GitHub), que eliminam a necessidade de manter e pagar por uma VM persistente (IaaS), garantindo o **Custo Zero Estrutural** e a automa√ß√£o do pipeline.

### 3.2. Acesso √†s Credenciais (Para Pr√≥ximos Testes)

Se voc√™ precisar do Endpoint ou das Keys para reconfigurar um teste futuro (local ou no CI/CD):

1.  Navegue at√© o Resource Group `RG-KAURA-DOC-AI` no Portal do Azure.
2.  Clique no servi√ßo **Document Intelligence** (o nome atualizado √© `kaura-doc-ai-service-05`).
3.  As credenciais estar√£o na se√ß√£o **Keys and Endpoint**.

### 3.3. Op√ß√£o: Limpeza Total (Excluir Resource Group)

Para zerar *absolutamente* o custo e deletar **TODOS** os recursos, execute este comando no Azure CLI. Ele excluir√° o Document Intelligence F0 e todos os demais recursos:

```bash
echo "Excluindo o Resource Group e TODOS os recursos dentro dele."
# Usamos o nome fixo para m√°xima robustez
az group delete --name RG-KAURA-DOC-AI --yes --no-wait
```

## ‚ö†Ô∏è Solu√ß√£o de Problemas Comuns (Troubleshooting):


### 1. Erro de Credencial do Azure (401 - Unauthorized / ResourceNotFound)
#### Problema

Ao tentar executar o script, o Azure retorna um erro como `401 Unauthorized` ou `ResourceNotFound` (Recurso N√£o Encontrado). Isso pode ocorrer mesmo ap√≥s criar o servi√ßo Document Intelligence.

#### Causa
O servi√ßo Doc AI (principalmente no Tier F0) pode n√£o liberar imediatamente o nome de recurso ap√≥s a exclus√£o, ou as credenciais antigas s√£o rejeitadas.

#### Solu√ß√£o
1.  **Crie um novo servi√ßo de Document Intelligence no Azure** (com um nome ligeiramente diferente, para evitar conflito).
2.  **Obtenha o novo ENDPOINT e KEY** (se√ß√£o 1.2 do SETUP).
3.  **Atualize as credenciais nas Secrets do GitHub** (`AZURE_FORM_RECOGNIZER_ENDPOINT` e `AZURE_FORM_RECOGNIZER_KEY`) e no arquivo `.env` (se estiver testando localmente).

---
### 2. Erro de Permiss√£o do GitHub (Remote Rejected)
#### Problema
Ao tentar enviar o arquivo de workflow (`.github/workflows/main.yml`) para o GitHub, voc√™ pode receber um erro: `refusing to allow a Personal Access Token to create or update workflow... without 'workflow' scope`.

#### Causa
O Personal Access Token (PAT) usado como "senha" para o comando `git push` n√£o possui a permiss√£o especial (`scope`) de **`workflow`**. Essa permiss√£o √© necess√°ria para modificar arquivos dentro da pasta `.github/workflows/`.

#### Solu√ß√£o
1.  Acesse as configura√ß√µes do seu PAT no GitHub (Settings -> Developer Settings -> Personal Access Tokens).
2.  Clique no token que voc√™ est√° usando (ou gere um novo se preferir).
3.  Na se√ß√£o **Scopes (Permiss√µes)**, certifique-se de que a caixa **`workflow`** esteja marcada, **al√©m** da permiss√£o `repo` que j√° deve estar selecionada.
4.  Clique em **Update Token** (Atualizar Token).
5.  Execute o comando `git push origin main` novamente, usando o Token atualizado como senha.

---

### 3. Erro de Depend√™ncia no CI/CD (`requirements.txt` n√£o encontrado)

#### Problema
O GitHub Actions falha na etapa `Install dependencies` com o erro: `ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'`.

#### Causa
O *workflow* do GitHub Actions espera o arquivo `requirements.txt` na raiz do projeto para saber quais bibliotecas instalar (`azure-ai-formrecognizer`, `python-dotenv`).

#### Solu√ß√£o
1.  **Crie o arquivo `requirements.txt`** na **raiz** do projeto.
2.  **Liste as depend√™ncias** do Python, uma por linha:
    ```
    python-dotenv
    azure-ai-formrecognizer
    ```
3.  Comite e envie o arquivo para o GitHub. O CI/CD ser√° acionado e encontrar√° as depend√™ncias.


